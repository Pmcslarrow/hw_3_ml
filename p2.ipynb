{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task\n",
    "\n",
    "Despite being a binary classification problem, problem 2 wants us to explore what happens if we use Mean Squared Error (MSE) as our loss function. Please recall that the Mean Squared Error is a loss function typically used for regression tasks, but we are attempting to use it for a classification task. The largest problem when using MSE is that it can greatly depend on the scale of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_prefix = 'spiral'\n",
    "# csv_file_prefix = 'two_gaussians'\n",
    "#csv_file_prefix = 'xor'\n",
    "#csv_file_prefix = 'center_surround'\n",
    "\n",
    "batch_size = 32\n",
    "# train_fraction = 0.8\n",
    "\n",
    "hidden_size = 64\n",
    "lr=0.001\n",
    "num_epochs = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "### NOTE TO SELF -- Are we supposed to fit_transform each dataset on its features, or only fit to the training data and transform train, test, and valid data based on training features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        super().__init__()\n",
    "        data = pd.read_csv(csv_file)\n",
    "        self.features = data.iloc[:, 1:]\n",
    "        self.labels = data.iloc[:, 0]\n",
    "\n",
    "        self.scaler = StandardScaler() # z-score all input features\n",
    "        self.features = self.scaler.fit_transform(self.features)\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        feature = torch.tensor(self.features[idx], dtype=torch.float32)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        one_hot_label = F.one_hot(label, num_classes=2).float() # one-hot encoding the labels so we can perform MSELoss on this... tensor([0., 1.])) is 1 and tensor([1., 0.])) is 0\n",
    "        return feature, one_hot_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SimpleDataset(csv_file=f'{csv_file_prefix}_train.csv')\n",
    "valid_dataset = SimpleDataset(csv_file=f'{csv_file_prefix}_valid.csv')\n",
    "test_dataset = SimpleDataset(csv_file=f'{csv_file_prefix}_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.f1 = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.f2 = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.f1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.f2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = train_dataset.features.shape[1] \n",
    "output_size = len(set(train_dataset.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNN(input_size, hidden_size, output_size)\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, valid_loader, num_epochs, criterion, optimizer):\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "\n",
    "        for features, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_train_loss += loss.item()\n",
    "\n",
    "        epoch_train_loss = running_train_loss / len(train_loader)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        running_valid_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for features, labels in valid_loader:\n",
    "                outputs = model(features)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                running_valid_loss += loss.item()\n",
    "\n",
    "        epoch_valid_loss = running_valid_loss / len(valid_loader)\n",
    "        valid_losses.append(epoch_valid_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_train_loss:.4f}, Valid Loss: {epoch_valid_loss:.4f}\")\n",
    "        \n",
    "    return train_losses, valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, valid_losses = train(model, train_loader, valid_loader, num_epochs, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "outputs\n",
    "tensor([[-0.0570,  1.0044],\n",
    "        [-0.4251,  1.4154],\n",
    "        [ 0.2088,  0.7987],\n",
    "\n",
    "predicted\n",
    "tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,\n",
    "        1, 1, 0, 0, 1, 1, 1, 1]) (NEED TO CONVERT INTO ONE_HOT)\n",
    "\n",
    "total_test\n",
    "32                \n",
    "\"\"\"\n",
    "\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    with torch.no_grad():\n",
    "        for features, one_hot_labels in test_loader:\n",
    "                outputs = model(features)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                \n",
    "                labels = torch.argmax(one_hot_labels, dim=1)\n",
    "\n",
    "                total_test += labels.size(0)\n",
    "                correct_test += (predicted == labels).sum().item()\n",
    "\n",
    "    test_accuracy = 100 * correct_test / total_test\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = test(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOSS CURVES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(range(1, (num_epochs) + 1)), train_losses, label='train loss')\n",
    "plt.plot(list(range(1, (num_epochs) + 1)), valid_losses, label='valid loss')\n",
    "plt.title(f'Loss Curves for the {csv_file_prefix.capitalize()} Dataset')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('cross entropy loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Decision Boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_min, x_max = train_dataset.features[:, 0].min(), train_dataset.features[:, 0].max()\n",
    "y_min, y_max = train_dataset.features[:, 1].min(), train_dataset.features[:, 1].max()\n",
    "\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
    "grid_points = torch.tensor(np.c_[xx.ravel(), yy.ravel()], dtype=torch.float32)\n",
    "\n",
    "with torch.no_grad():\n",
    "    Z = model(grid_points)\n",
    "    Z = Z.argmax(dim=1).numpy()  \n",
    "\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.contourf(xx, yy, Z, alpha=0.5, cmap='viridis')\n",
    "\n",
    "for inputs, one_hot_labels in train_loader:\n",
    "    inputs = inputs.numpy()\n",
    "    labels = one_hot_labels.argmax(dim=1).numpy()  # CONVERTING ONE HOT BACK INTO LABEL\n",
    "    plt.scatter(inputs[:, 0], inputs[:, 1], c=labels, edgecolor='k', cmap='coolwarm', marker='o')\n",
    "\n",
    "plt.title(\"Decision Boundary with Training Data Points\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw_3_ml.env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
