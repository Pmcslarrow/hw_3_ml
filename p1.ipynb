{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_prefix = 'spiral'\n",
    "# csv_file_prefix = 'two_gaussians'\n",
    "# csv_file_prefix = 'xor'\n",
    "# csv_file_prefix = 'center_surround'\n",
    "\n",
    "batch_size = 32\n",
    "hidden_size = 64\n",
    "lr=0.001\n",
    "num_epochs = 200\n",
    "lambda_val = 0.002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, scaler, transform=None):\n",
    "        super().__init__()\n",
    "        self.features = df.iloc[:, 1:]\n",
    "        self.labels = df.iloc[:, 0]\n",
    "        \n",
    "        self.features = scaler.transform(self.features)\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        feature = torch.tensor(self.features[idx], dtype=torch.float32)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return feature, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importing the data and scaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()  # z-score all input features\n",
    "\n",
    "train_df = pd.read_csv(f'{csv_file_prefix}_train.csv')\n",
    "\n",
    "train_features = train_df.iloc[:, 1:]\n",
    "features = scaler.fit(train_features)\n",
    "\n",
    "valid_df = pd.read_csv(f'{csv_file_prefix}_valid.csv')\n",
    "test_df = pd.read_csv(f'{csv_file_prefix}_test.csv')\n",
    "\n",
    "train_dataset = SimpleDataset(df=train_df, scaler=scaler)\n",
    "valid_dataset = SimpleDataset(df=valid_df, scaler=scaler)\n",
    "test_dataset = SimpleDataset(df=test_df, scaler=scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "##### Converting into DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feed Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.f1 = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.f2 = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.f1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.f2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "##### Initializing model / cost function / optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = train_dataset.features.shape[1] \n",
    "output_size = len(set(train_dataset.labels))\n",
    "\n",
    "model_no_reg = SimpleNN(input_size, hidden_size, output_size)\n",
    "model_with_reg = SimpleNN(input_size, hidden_size, output_size)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_no_reg = torch.optim.Adam(model_no_reg.parameters(), lr=lr)\n",
    "optimizer_with_reg = torch.optim.Adam(model_with_reg.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training the model\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img alt=\"L2-regularization equation\" src=\"./l2_reg.png\" />\n",
    "  <div><i>L2-Regularization</i></div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, valid_loader, num_epochs, criterion, optimizer, l2_regularization=False):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: Pytorch nn.Module model\n",
    "    train_loader: training set as a DataLoader()\n",
    "    valid_loader: validation set as a DataLoader()\n",
    "    num_epochs: Number of epochs\n",
    "    criterion: Loss function\n",
    "    optimizer: Optimization function\n",
    "    l2_regularization: Boolean value representing if you want to use L2-regularization\n",
    "    \"\"\"\n",
    "    \n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "\n",
    "        for features, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            if l2_regularization:\n",
    "                reg = lambda_val * torch.sum(model.f1.weight ** 2)\n",
    "                loss = loss + reg\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_train_loss += loss.item()\n",
    "\n",
    "        epoch_train_loss = running_train_loss / len(train_loader)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        running_valid_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for features, labels in valid_loader:\n",
    "                outputs = model(features)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                running_valid_loss += loss.item()\n",
    "\n",
    "        epoch_valid_loss = running_valid_loss / len(valid_loader)\n",
    "        valid_losses.append(epoch_valid_loss)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_train_loss:.4f}, Valid Loss: {epoch_valid_loss:.4f}\")\n",
    "        \n",
    "    return train_losses, valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0b/t_zq87x958lc5wvb065nb89m0000gn/T/ipykernel_64476/246721670.py:16: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  label = torch.tensor(self.labels[idx], dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/200, Train Loss: 0.4800, Valid Loss: 0.5096\n",
      "Epoch 20/200, Train Loss: 0.4530, Valid Loss: 0.4701\n",
      "Epoch 30/200, Train Loss: 0.4241, Valid Loss: 0.4507\n",
      "Epoch 40/200, Train Loss: 0.4326, Valid Loss: 0.4309\n",
      "Epoch 50/200, Train Loss: 0.4107, Valid Loss: 0.4091\n",
      "Epoch 60/200, Train Loss: 0.3788, Valid Loss: 0.3887\n",
      "Epoch 70/200, Train Loss: 0.3874, Valid Loss: 0.3688\n",
      "Epoch 80/200, Train Loss: 0.3262, Valid Loss: 0.3467\n",
      "Epoch 90/200, Train Loss: 0.2981, Valid Loss: 0.3226\n",
      "Epoch 100/200, Train Loss: 0.3143, Valid Loss: 0.3004\n",
      "Epoch 110/200, Train Loss: 0.2892, Valid Loss: 0.2797\n",
      "Epoch 120/200, Train Loss: 0.2643, Valid Loss: 0.2563\n",
      "Epoch 130/200, Train Loss: 0.2507, Valid Loss: 0.2353\n",
      "Epoch 140/200, Train Loss: 0.2224, Valid Loss: 0.2161\n",
      "Epoch 150/200, Train Loss: 0.2085, Valid Loss: 0.1981\n",
      "Epoch 160/200, Train Loss: 0.1895, Valid Loss: 0.1835\n",
      "Epoch 170/200, Train Loss: 0.1839, Valid Loss: 0.1695\n",
      "Epoch 180/200, Train Loss: 0.1950, Valid Loss: 0.1570\n",
      "Epoch 190/200, Train Loss: 0.1722, Valid Loss: 0.1449\n",
      "Epoch 200/200, Train Loss: 0.1643, Valid Loss: 0.1353\n",
      "Epoch 10/200, Train Loss: 0.5205, Valid Loss: 0.5015\n",
      "Epoch 20/200, Train Loss: 0.4767, Valid Loss: 0.4606\n",
      "Epoch 30/200, Train Loss: 0.4574, Valid Loss: 0.4447\n",
      "Epoch 40/200, Train Loss: 0.4246, Valid Loss: 0.4215\n",
      "Epoch 50/200, Train Loss: 0.4181, Valid Loss: 0.4008\n",
      "Epoch 60/200, Train Loss: 0.3971, Valid Loss: 0.3812\n",
      "Epoch 70/200, Train Loss: 0.4120, Valid Loss: 0.3618\n",
      "Epoch 80/200, Train Loss: 0.3604, Valid Loss: 0.3427\n",
      "Epoch 90/200, Train Loss: 0.3634, Valid Loss: 0.3253\n",
      "Epoch 100/200, Train Loss: 0.3644, Valid Loss: 0.3070\n",
      "Epoch 110/200, Train Loss: 0.3306, Valid Loss: 0.2885\n",
      "Epoch 120/200, Train Loss: 0.2995, Valid Loss: 0.2693\n",
      "Epoch 130/200, Train Loss: 0.2980, Valid Loss: 0.2539\n",
      "Epoch 140/200, Train Loss: 0.2832, Valid Loss: 0.2363\n",
      "Epoch 150/200, Train Loss: 0.2644, Valid Loss: 0.2215\n",
      "Epoch 160/200, Train Loss: 0.2562, Valid Loss: 0.2085\n",
      "Epoch 170/200, Train Loss: 0.2317, Valid Loss: 0.1943\n",
      "Epoch 180/200, Train Loss: 0.2483, Valid Loss: 0.1811\n",
      "Epoch 190/200, Train Loss: 0.2396, Valid Loss: 0.1709\n",
      "Epoch 200/200, Train Loss: 0.2208, Valid Loss: 0.1611\n"
     ]
    }
   ],
   "source": [
    "train_losses_no_reg, valid_losses_no_reg = train(model_no_reg, train_loader, valid_loader, num_epochs, criterion, optimizer_no_reg, l2_regularization=False)\n",
    "train_losses_with_reg, valid_losses_with_reg = train(model_with_reg, train_loader, valid_loader, num_epochs, criterion, optimizer_with_reg, l2_regularization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing the model (with accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: Pytorch nn.Module model\n",
    "    test_loader: test set as a DataLoader()\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    with torch.no_grad():\n",
    "        for features, labels in test_loader:\n",
    "            outputs = model(features)            \n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_test += labels.size(0)\n",
    "            correct_test += (predicted == labels).sum().item()\n",
    "\n",
    "    test_accuracy = 100 * correct_test / total_test\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = test(model_no_reg, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Loss Curves and Decision Boundaries\n",
    "\n",
    "Recall that L2-Regularization is a technique used to limit overfitting the training set by increases bias between the training set and test sets while decreasing the variance. You can see the difference in plots below that the training loss performance is not as good, but the validation loss still has low loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curves(train_losses_no_reg, valid_losses_no_reg, postfix=\"No regularization\")\n",
    "plot_loss_curves(train_losses_with_reg, valid_losses_with_reg, postfix=\"With regularization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = test(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DECISION BOUNDARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundaries(model_no_reg, title=f\"-Unregularized- Dataset: {csv_file_prefix.capitalize()}, Nodes: {hidden_size}, Cost Function: MCE\")\n",
    "plot_decision_boundaries(model_with_reg, title=f\"-Regularized- Dataset: {csv_file_prefix.capitalize()}, Nodes: {hidden_size}, Cost Function: MCE\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw_3_ml.env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
